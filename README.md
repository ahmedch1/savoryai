# ğŸ½ï¸ SavoryAI

AI-powered food recommendation system with a modern React web interface and FastAPI backend.  
Uses vector similarity search (ChromaDB) and an optional local LLM (Ollama) for natural-language food advice â€” **completely free, no API keys required**.

![Python](https://img.shields.io/badge/Python-3.12-blue)
![React](https://img.shields.io/badge/React-18-61dafb)
![FastAPI](https://img.shields.io/badge/FastAPI-0.110-009688)
![License](https://img.shields.io/badge/License-MIT-green)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Semantic Food Search** | Find foods by meaning, not just keywords â€” "something light and healthy" works |
| **Cuisine & Calorie Filters** | Narrow results by cuisine type and maximum calories |
| **AI Chat (RAG)** | Ask food questions in natural language; answers are grounded in the dataset |
| **Professional Web UI** | Dark-themed React interface with animations, score badges & responsive layout |
| **100 % Local** | Runs entirely on your machine â€” no cloud, no API keys, no cost |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        HTTP        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend  â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  FastAPI Backend      â”‚
â”‚  (Vite Â· :5174)  â”‚   JSON REST API    â”‚  (Uvicorn Â· :8000)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  shared_functions.py  â”‚
                                        â”‚  ChromaDB + Embeddingsâ”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚  FoodDataSet.json (185 items)â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚ (optional)
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚  Ollama / Mistralâ”‚
                                          â”‚  Local LLM       â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** and **pip**
- **Node.js 18+** and **npm**
- (Optional) **Ollama** â€” for AI chat; search works without it

### 1. Clone & install Python dependencies

```bash
git clone <your-repo-url> savoryai && cd savoryai

python3 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

### 2. Install frontend dependencies

```bash
cd frontend
npm install
cd ..
```

### 3. (Optional) Set up Ollama for AI chat

```bash
# Download from https://ollama.ai, then:
ollama serve          # Terminal 1
ollama pull mistral   # Terminal 2
```

See [SETUP_OLLAMA.md](SETUP_OLLAMA.md) for detailed instructions.  
Search works perfectly **without** Ollama; only the "Ask AI" chat tab needs it.

### 4. Start the app (two terminals)

**Terminal 1 â€” Backend:**

```bash
source venv/bin/activate
uvicorn backend.app:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 â€” Frontend:**

```bash
cd frontend
npm run dev
```

Open **http://localhost:5174** in your browser. ğŸ‰

---

## ğŸ”Œ API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/api/health` | Health check â€” returns status, item count, Ollama availability |
| `POST` | `/api/search` | Semantic food search with optional filters |
| `POST` | `/api/chat` | RAG-powered AI chat about food |

### Search example

```bash
curl -s http://localhost:8000/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "spicy chicken", "cuisine_filter": "Indian", "max_calories": 500, "n_results": 3}' | python3 -m json.tool
```

### Chat example

```bash
curl -s http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What are some healthy breakfast options?"}' | python3 -m json.tool
```

---

## ğŸ“‚ Project Structure

```
savoryai/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app.py                     # FastAPI application (search + chat endpoints)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ favicon.svg
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Main React component (Search / Ask AI tabs)
â”‚   â”‚   â”œâ”€â”€ Logo.jsx               # SVG logo component
â”‚   â”‚   â”œâ”€â”€ api.js                 # Fetch wrapper for backend API
â”‚   â”‚   â”œâ”€â”€ main.jsx               # React entry point
â”‚   â”‚   â””â”€â”€ styles.css             # Dark design system (glass-morphism, animations)
â”‚   â”œâ”€â”€ .env                       # VITE_API_BASE_URL
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js             # Port 5174, strictPort
â”œâ”€â”€ shared_functions.py            # ChromaDB client, data loading, search logic
â”œâ”€â”€ FoodDataSet.json               # 185 curated food items (sole data source)
â”œâ”€â”€ interactive_search.py          # CLI food search (standalone)
â”œâ”€â”€ enhanced_rag_chatbot.py        # CLI RAG chatbot (standalone)
â”œâ”€â”€ system_comparison.py           # Benchmarking utility
â”œâ”€â”€ advanced_search.py             # Advanced CLI search
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ SETUP_OLLAMA.md                # Ollama installation guide
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ—‚ï¸ Dataset

**FoodDataSet.json** contains **185** curated food items spanning **20+ cuisines** including Indian, Italian, Japanese, Mexican, Thai, Mediterranean, and more.

Each item includes:

| Field | Example |
|-------|---------|
| `food_name` | Chicken Tikka Masala |
| `food_description` | Tender chicken in creamy tomato-based sauceâ€¦ |
| `cuisine_type` | Indian |
| `food_calories_per_serving` | 350 |
| `food_ingredients` | ["chicken", "yogurt", "tomato", â€¦] |
| `cooking_method` | Grilled then simmered |
| `food_health_benefits` | High protein, rich in vitaminsâ€¦ |
| `food_features` | ["spicy", "creamy", "rich"] |

All search and chat results come **exclusively** from this dataset.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React 18, Vite 5, CSS custom properties |
| Backend | FastAPI, Uvicorn, Pydantic |
| Embeddings | Sentence Transformers (`all-MiniLM-L6-v2`) |
| Vector DB | ChromaDB |
| LLM (optional) | Ollama with Mistral |
| Language | Python 3.12, JavaScript (ES2022) |

---

## ğŸ”§ Troubleshooting

| Problem | Solution |
|---------|----------|
| Port 5174 in use | Change port in `frontend/vite.config.js` |
| "Ollama unavailable" in chat | Start Ollama: `ollama serve` then `ollama pull mistral` |
| CORS errors | Backend already includes CORS middleware for `localhost:5174` |
| ChromaDB telemetry warning | Cosmetic only â€” does not affect functionality |
| No search results | Ensure backend is running and check the health endpoint |

---

## ğŸ“„ License

[MIT](LICENSE)
